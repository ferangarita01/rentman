'use client';

import React, { createContext, useContext, useState, useRef, useCallback, useEffect } from 'react';
import { useRouter } from 'next/navigation';
import { useAuth } from './AuthContext';
import { NativeLog } from '@/lib/nativeLogger';

// WebSocket server URL
// WebSocket server URL
const BACKEND_URL = process.env.NEXT_PUBLIC_BACKEND_URL || 'http://192.168.80.11:8081';
// Remove 'http://' or 'https://' to get host
const HOST = BACKEND_URL.replace(/^https?:\/\//, '');
const WS_URL = `ws://${HOST}`;

// Dual logging: console + native Android
const log = {
    info: (msg: string) => { console.log(msg); NativeLog.info(msg); },
    warn: (msg: string) => { console.warn(msg); NativeLog.warn(msg); },
    error: (msg: string) => { console.error(msg); NativeLog.error(msg); },
    debug: (msg: string) => { console.log(msg); NativeLog.debug(msg); },
};

interface SarahContextType {
    isActive: boolean;
    setIsActive: (active: boolean) => void; // âœ… Add this line
    isConnected: boolean;
    isListening: boolean;
    isReady: boolean;
    isSpeaking: boolean;
    agentResponse: string;
    toggleSarah: () => void;
    disconnect: () => void;
    sendUserAction: (action: string, data?: any) => void;
    isFullPageMode: boolean;
    setIsFullPageMode: (isFull: boolean) => void;
    mode: string; // âœ… New mode state
    setMode: (mode: string) => void; // âœ… New setter
}

const SarahContext = createContext<SarahContextType | null>(null);

export function useSarah() {
    const context = useContext(SarahContext);
    if (!context) {
        throw new Error('useSarah must be used within SarahProvider');
    }
    return context;
}

export function SarahProvider({ children }: { children: React.ReactNode }) {
    const { user } = useAuth();
    const router = useRouter();

    const [isActive, setIsActive] = useState(false);
    const [isConnected, setIsConnected] = useState(false);
    const [isListening, setIsListening] = useState(false);
    const [isReady, setIsReady] = useState(false);
    const [agentResponse, setAgentResponse] = useState('');
    const [isFullPageMode, setIsFullPageMode] = useState(false);
    const [mode, setMode] = useState('normal'); // âœ… Default mode

    const wsRef = useRef<WebSocket | null>(null);
    const audioContextRef = useRef<AudioContext | null>(null);
    const processorRef = useRef<ScriptProcessorNode | null>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const isReadyRef = useRef(false);
    const isSpeakingRef = useRef(false);
    const playbackCtxRef = useRef<AudioContext | null>(null);
    const nextPlayTimeRef = useRef(0);
    const currentSourceRef = useRef<AudioBufferSourceNode | null>(null);
    const activeAudioTimeoutRef = useRef<any>(null);

    // Connect when activated
    useEffect(() => {
        log.info('ðŸ”„ [Sarah] isActive changed: ' + isActive + ' user: ' + !!user);

        if (isActive && user && !wsRef.current) {
            // Only connect if not already connected
            log.info('âœ… [Sarah] Conditions met, calling connectAgent');
            connectAgent();
        }
        // âœ… REMOVED: Auto-disconnect when isActive becomes false
        // This allows Sarah to stay connected when navigating between tabs
        // User must explicitly close Sarah via the close button

        return () => {
            // Cleanup only if explicitly deactivated
            // Don't auto-disconnect on unmount
        };
    }, [isActive, user]);

    const connectAgent = async () => {
        log.info('ðŸ”µ [Sarah] connectAgent called');
        log.info('ðŸ”µ [Sarah] isActive: ' + isActive);
        log.info('ðŸ”µ [Sarah] user: ' + (user?.id || 'null'));
        log.info('ðŸ”µ [Sarah] wsRef.current: ' + (wsRef.current ? 'exists' : 'null'));

        if (typeof window === 'undefined' || wsRef.current) {
            log.warn('âš ï¸ [Sarah] Skipping connect - window undefined or ws exists');
            return;
        }

        log.info('ðŸ”µ [Sarah] Setting response: Conectando...');
        setAgentResponse('ðŸ”„ Conectando...');

        try {
            const url = `${WS_URL}?mode=${mode}`; // âœ… Append mode to URL
            log.info('ðŸ”µ [Sarah] Creating WebSocket: ' + url);
            const ws = new WebSocket(url);
            ws.binaryType = 'arraybuffer';
            wsRef.current = ws;

            ws.onopen = () => {
                log.info('âœ… [Sarah] WebSocket opened!');
                setAgentResponse('âœ… Conectado');
                setIsConnected(true);

                ws.send(JSON.stringify({
                    type: 'init',
                    userId: user?.id || 'guest_user'
                }));
                log.info('ðŸ”µ [Sarah] Init message sent, waiting for ready...');
                // âš ï¸ DON'T call startListening() here - wait for 'ready' message
            };

            ws.onmessage = (event: MessageEvent) => {
                if (event.data instanceof ArrayBuffer) {
                    const view = new Uint8Array(event.data);
                    log.debug('ðŸŽµ [Sarah] Audio data received. First byte: ' + view[0] + ' Size: ' + event.data.byteLength);
                    if (view[0] === 0x02) {
                        log.info('ðŸ”Š [Sarah] Calling playAudio with ' + (event.data.byteLength - 1) + ' bytes');
                        isSpeakingRef.current = true;
                        if (!isReadyRef.current) {
                            setIsReady(true);
                            isReadyRef.current = true;
                            setAgentResponse('Escuchando...');
                        }
                        playAudio(event.data.slice(1));
                    } else {
                        log.warn('âš ï¸ [Sarah] Unknown audio byte: ' + view[0]);
                    }
                } else {
                    try {
                        const msg = JSON.parse(event.data);

                        if (msg.type === 'debug') {
                            setAgentResponse(`ðŸ”§ ${msg.message}`);
                            return;
                        }

                        if (msg.type === 'ready') {
                            setIsReady(true);
                            isReadyRef.current = true;
                            setAgentResponse('Escuchando...');
                            // ðŸŽ¤ NOW start listening after backend confirms ready
                            log.info('ðŸ”µ [Sarah] Backend ready, starting microphone...');
                            startListening();
                            return;
                        }

                        if (msg.type === 'interrupt') {
                            if (currentSourceRef.current) {
                                try { currentSourceRef.current.stop(); } catch (e) { }
                                currentSourceRef.current = null;
                            }
                            isSpeakingRef.current = false;
                            return;
                        }

                        // Handle navigation from Sarah
                        if (msg.type === 'app_control') {
                            if (msg.command === 'navigate') {
                                setAgentResponse(`ðŸš€ Navegando a ${msg.path}...`);
                                router.push(msg.path);
                            }
                        }

                        // Handle dynamic UI rendering (TODO: implement render system)
                        if (msg.type === 'render_ui') {
                            log.info('Render UI: ' + JSON.stringify(msg));
                            // Will dispatch event for current page to handle
                            window.dispatchEvent(new CustomEvent('sarah_render', { detail: msg }));
                        }

                        // Handle scheduled notifications from Sarah
                        if (msg.type === 'schedule_notification') {
                            console.log('ðŸ“… Schedule notification:', msg.notification);
                            window.dispatchEvent(new CustomEvent('schedule_notification', {
                                detail: msg.notification
                            }));
                        }

                        // Handle Onboarding Completion
                        if (msg.type === 'onboarding_complete') {
                            console.log('ðŸŽ‰ Onboarding event received from backend');
                            window.dispatchEvent(new CustomEvent('onboarding_complete'));
                        }

                    } catch (e) {
                        console.error('Parse error:', e);
                    }
                }
            };

            ws.onclose = () => {
                log.warn('ðŸ”´ [Sarah] WebSocket closed');
                setIsConnected(false);
                setIsReady(false);
                setIsListening(false);
                wsRef.current = null;
            };

            ws.onerror = (error) => {
                log.error('âŒ [Sarah] WebSocket error: ' + JSON.stringify(error));
                setAgentResponse('âŒ Error de conexiÃ³n');
            };

        } catch (e) {
            log.error('âŒ [Sarah] Connection error: ' + (e as Error).message);
            setAgentResponse('âŒ Error al crear conexiÃ³n');
        }
    };

    const disconnectAgent = useCallback(() => {
        log.info('ðŸ”´ [Sarah] disconnectAgent called');

        // 1. WebSocket (Close immediately if open)
        if (wsRef.current) {
            // Prevent onclose/onerror from firing after manual disconnect
            wsRef.current.onclose = null;
            wsRef.current.onerror = null;
            wsRef.current.onmessage = null;

            if (wsRef.current.readyState === WebSocket.OPEN || wsRef.current.readyState === WebSocket.CONNECTING) {
                wsRef.current.close();
            }
            wsRef.current = null;
        }

        // 2. Microphone Stream (Stop tracks)
        if (streamRef.current) {
            streamRef.current.getTracks().forEach(t => t.stop());
            streamRef.current = null;
        }

        // 3. Audio Contexts (Close safely)
        // Note: .close() returns a promise, we don't await because unmount shouldn't block
        if (audioContextRef.current) {
            if (audioContextRef.current.state !== 'closed') {
                audioContextRef.current.close().catch(e => console.warn('Error closing Mic Context:', e));
            }
            audioContextRef.current = null;
        }

        // 4. Playback Context
        if (playbackCtxRef.current) {
            if (playbackCtxRef.current.state !== 'closed') {
                playbackCtxRef.current.close().catch(e => console.warn('Error closing Playback Context:', e));
            }
            playbackCtxRef.current = null;
        }

        // 5. Timers
        if (activeAudioTimeoutRef.current) {
            clearTimeout(activeAudioTimeoutRef.current);
            activeAudioTimeoutRef.current = null;
        }

        // 6. Stop processor
        if (processorRef.current) {
            processorRef.current.disconnect();
            processorRef.current.onaudioprocess = null; // Kill loop
            processorRef.current = null;
        }

        // Reset State
        // Don't set isActive to false here - let the caller control that
        setIsListening(false);
        setIsConnected(false);
        setIsReady(false);
        isReadyRef.current = false;
        isSpeakingRef.current = false; // Reset speaking state
        if (currentSourceRef.current) {
            try { currentSourceRef.current.stop(); } catch (e) { }
            currentSourceRef.current = null;
        }
        setAgentResponse('');
    }, []);

    const startListening = async () => {
        console.log('ðŸŽ¤ [Sarah] startListening called');

        try {
            if (typeof window === 'undefined') {
                console.log('âš ï¸ [Sarah] Window is undefined, skipping');
                return;
            }

            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                setAgentResponse('âš ï¸ Microphone requires HTTPS or localhost');
                console.error('Media devices not available');
                return;
            }

            // ðŸŽ¤ NATIVE AEC CONFIGURATION (Android Optimized)
            // We use 'sampleRate: undefined' to let the OS/Hardware handle the native stream
            // Then we software-downsample to 16kHz for Gemini.
            console.log('ðŸŽ¤ Requesting NATIVE echo-cancelled microphone access...');
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                    sampleRate: undefined // Let OS decide (Critical for Android 16k crash fix)
                }
            });
            console.log('âœ… [Sarah] Microphone access granted!');
            streamRef.current = stream;

            // processing context (16kHz target)
            // If we can't get 16k natively, we handle it in the processor
            console.log('ðŸŽ›ï¸ [Sarah] Creating AudioContext...');
            const audioContext = new AudioContext({ sampleRate: 16000 });
            audioContextRef.current = audioContext;
            console.log(`âœ… [Sarah] AudioContext created. Sample rate: ${audioContext.sampleRate}Hz`);

            const source = audioContext.createMediaStreamSource(stream);
            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            processorRef.current = processor;

            processor.onaudioprocess = (e) => {
                // Check state
                if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;
                if (!isReadyRef.current) return;
                if (isSpeakingRef.current) return; // Half-duplex (mute self when agent speaks)

                const inputData = e.inputBuffer.getChannelData(0);

                // ðŸ”Š DOWNSAMPLING LOGIC (If needed, although AudioContext(16k) handles most)
                // We clamp and convert Float32 -> Int16
                const pcm16 = new Int16Array(inputData.length);
                let sumSq = 0;

                for (let i = 0; i < inputData.length; i++) {
                    const s = Math.max(-1, Math.min(1, inputData[i]));
                    sumSq += s * s;
                    pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                }

                // Noise Gate (Simple)
                const rms = Math.sqrt(sumSq / inputData.length);
                if (rms < 0.002) return; // Silence

                // Send Packet
                const packet = new Uint8Array(1 + pcm16.byteLength);
                packet[0] = 0x01; // Opcode
                packet.set(new Uint8Array(pcm16.buffer), 1);
                wsRef.current.send(packet);
            };

            source.connect(processor);
            processor.connect(audioContext.destination);
            setIsListening(true);
            console.log('ðŸŽ¤ [Sarah] Audio processing pipeline ready!');

        } catch (e: any) {
            console.error('âŒ [Sarah] Microphone error:', e);
            console.error('âŒ [Sarah] Error name:', e.name);
            console.error('âŒ [Sarah] Error message:', e.message);
            setAgentResponse('âŒ Error con micrÃ³fono');
        }
    };

    const playAudio = async (audioData: ArrayBuffer) => {
        console.log('ðŸŽ§ [Sarah] playAudio called with', audioData.byteLength, 'bytes');
        try {
            if (!playbackCtxRef.current) {
                playbackCtxRef.current = new AudioContext({ sampleRate: 24000 });
                console.log('ðŸŽ§ [Sarah] AudioContext created. Sample rate:', playbackCtxRef.current.sampleRate);
            }
            const ctx = playbackCtxRef.current;

            const int16Array = new Int16Array(audioData);
            const float32Array = new Float32Array(int16Array.length);
            for (let i = 0; i < int16Array.length; i++) {
                float32Array[i] = int16Array[i] / 32768;
            }

            // Gemini sends 24kHz, create buffer at 24kHz then let browser resample
            const audioBuffer = ctx.createBuffer(1, float32Array.length, 24000);
            audioBuffer.getChannelData(0).set(float32Array);

            const source = ctx.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(ctx.destination);

            const now = ctx.currentTime;
            const startTime = Math.max(now, nextPlayTimeRef.current);
            source.start(startTime);
            nextPlayTimeRef.current = startTime + audioBuffer.duration;
            currentSourceRef.current = source;

            if (activeAudioTimeoutRef.current) {
                clearTimeout(activeAudioTimeoutRef.current);
            }

            // FIX: Calculate timeout based on the TOTAL queue end time, not just this chunk.
            // This ensures microphone stays muted (isSpeaking=true) until the entire sentence finishes.
            const timeUntilQueueEnds = Math.max(0, nextPlayTimeRef.current - ctx.currentTime);
            const durationMs = timeUntilQueueEnds * 1000;

            activeAudioTimeoutRef.current = setTimeout(() => {
                console.log('ðŸ”‡ Agent finished speaking (Queue empty)');
                isSpeakingRef.current = false;
            }, durationMs + 300); // +300ms buffer for safety

        } catch (e) {
            console.error('âŒ [Sarah] Audio play error:', e);
        }
    };

    const sendUserAction = useCallback((action: string, data: any = {}) => {
        if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
            wsRef.current.send(JSON.stringify({
                type: 'user_action',
                action,
                data
            }));
            console.log('ðŸ“¤ Sent User Action:', action, data);
        } else {
            console.warn('âš ï¸ Cannot send user action, WS not connected:', action);
        }
    }, []);

    const toggleSarah = useCallback(() => {
        setIsActive(prev => !prev);
    }, []);

    return (
        <SarahContext.Provider value={{
            isActive,
            setIsActive,
            isConnected,
            isListening,
            isReady,
            isSpeaking: isSpeakingRef.current,
            agentResponse,
            toggleSarah,
            disconnect: disconnectAgent,
            sendUserAction,
            isFullPageMode,
            setIsFullPageMode,
            mode, // âœ…
            setMode // âœ…
        }}>
            {children}
        </SarahContext.Provider>
    );
}
